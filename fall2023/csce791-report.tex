\documentclass[11pt, oneside]{article}   	% use "amsart" instead of "article" for AMSLaTeX format
%\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.
%\geometry{letterpaper}                   		% ... or a4paper or a5paper or ... 
%\geometry{landscape}                		% Activate for rotated page geometry
%\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent

\usepackage{geometry}
 \geometry{
 a4paper,
 total={170mm,257mm},
 left=20mm,
 top=22mm,
 bottom=22mm
 }

\usepackage{graphicx}				% Use pdf, png, jpg, or epsÂ§ with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex		
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{fancyhdr}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{enumerate}
\usepackage{arcs}
\usepackage{cancel}
\usepackage{xfrac}
\usepackage{amsthm}
\usepackage{gensymb}
\usepackage{xspace}
\usepackage{array}
\usepackage{tabularx}
\usepackage{url}
\usepackage{hyperref}
%\usepackage{ctex}

%SetFonts

%SetFonts

\usepackage[inline]{asymptote}


\pagestyle{fancy}
\fancyhf{}
\rhead{Zifei (David) Zhong, \url{zhongz@email.sc.edu}}
\lhead{\leftmark}
\lfoot{\href{https://github.com/zfzhong/csce791}{github.com/zfzhong/csce791/fall2023}}
\rfoot{\thepage}

\title{CSCE 791 Course Report}
\author{Zifei (David) Zhong}
%\date{}							% Activate to display a given date or no date

\newcommand{\latex}{\LaTeX\xspace}


\begin{document}
\maketitle

This file includes all my course reports for CSCE 791 for the semester
of Fall 2023. I will write a summary for each talk presented in the
seminar. I will also include interesting questions raised by the
audience, as well as corresponding responses from the speaker.\\

\begin{center}
\begin{tabularx}{0.65\textwidth}{r X}
\textbf{Course:} & CSCE 791: Seminar in Advances in Computing\\
\textbf{Location:} & Storey Innovation Center 1400\\
\textbf{Time:} & Friday 2:20pm - 3:10pm\\
\textbf{Semester:} & Fall 2023
\end{tabularx}
\end{center}

\newpage
\section{Talk on August 25, 2023}
Open remarks.

\newpage
\section{Talk on September 1, 2023}
\begin{tabularx} {\textwidth}{r X}
\textbf{Topic}: & AI research in the era of ChatGPT \\
\textbf{Speaker:} & Dr. Biplav Srivastava \\
\end{tabularx}

\subsection{Summary}
Dr. Srivastava introduced the evolving of his AI research projects in the era of ChatGPT. He firstly introduced Plansformer, a system that solves planning tasks. Later, they developed Plansformer in to a new system with architecture call The SOFAI Planner, which incorporates LLM to help solve planning tasks.

Through his research experience, Dr. Srivastava conveyed the message that LLMs are exciting but not reliable, and thus it won't be directly applied to solve practical problems. However, Dr. Srivastava suggested that using LLMs as an efficient way to test research ideas, and then more trustworthy and reliable applications can be built based on what LLMs generate.

He mentioned that ChatGPT responds with different answers for similar questions, and that might cause people not using it because it's behavior is not very predictable (which might be considered as faulty). I would say that for some questions there is no exact answer, and ChatGPT is smart in answering these questions.

\newpage
\section{Talk on September 8, 2023}
\begin{tabularx} {\textwidth}{r X}
\textbf{Topic}: & Resource-Aware Approximate Dynamic Programming and Reinforcement Learning for Optimal Control of Dynamic Cyber-Physical Systems \\
\textbf{Speaker:} & Dr. Avimanyu Sahoo\\
\end{tabularx}

\subsection{Summary}
Dr. Sahoo started his talk by introducing control of dynamic systems, with NASA's Kepler spacecraft as an example. He then proceeded to present the challenges existing in control of cyber-physical systems: bandwidth, computation capability, and uncertainties. He spent most of the talk discussing the possible solutions: adaptive control, neural network control, approximate dynamic programming, and reinforcement learning based optimal control.

Throughout detailed discussion of these solutions, Dr. Sahoo pointed out that these solutions have different resource constraints. Since traditional iterative solutions are computationally expensive, resource-aware control is designed to address the problem. Dr. Sahoo discussed resource-aware self-learning optimal control schemes. 

A lot of mathematical formulas were introduced in the talk. I didn't grasp the details of them. The high level ideas of resource-aware self-learning optimal control are quite interesting. 


\newpage
\section{Talk on September 15, 2023}
\begin{tabularx} {\textwidth}{r X}
\textbf{Topic}: & Trustworthy Artificial Intelligence Using Knowledge-powered CREST Framework \\
\textbf{Speaker:} & Dr. Manas Gaur\\
\end{tabularx}

Dr. Gaur introduced his Knowledge-powered CREST Framework for helping Large Language Models (LLM) evolve to better human lives. He pointed out that LLMs's understanding of language, while impressive, is actually shallow. He then argued that LLMs would never approximate human intelligence if trained on language alone.

To this end, he proposed the CREST framework that would help LLM evolve better. The CREST framework includes features named Consistency, Reliability, Explainability, etc. He spent a fair amount of time introducing the details of each feature and how to evaluate these features with respect to LLMs.

I didn't get much details of each of the features. I feel the CREST framework is quite needed and useful in developing LLMs.



\newpage
\section{Talk on September 22, 2023}
\begin{tabularx} {\textwidth}{r X}
\textbf{Topic}: & Towards Automotive Radar Networks for Enhanced Detection/Cognition \\
\textbf{Speaker:} & Dr. Sumit Roy\\
\end{tabularx}

Dr. Roy started his talk by introducing traditional signal processing techniques which has been widely used together with data collected by TI AWR radars to identify the movement of objects. He then pointed out that multi-radar interference could be a big issue in data processing. He then continued to introducing the challenges of applying radar to the autonomous driving context.

He introduced his research work on Vehicular SAR, a system that brings low-cost target detection and localization. He presented both simulation result and experiment result to validate the effectiveness of his proposal.

There are some interesting details in his talk. For example, how they improve the time complexity of their algorithm to make it applicable to scenarios which require  real-time processing.

\newpage
\section{Talk on September 29, 2023}
\begin{tabularx} {\textwidth}{r X}
\textbf{Topic}: & Bring IoT into Daily Life \\
\textbf{Speaker:} & Taiting Lu\\
\end{tabularx}

A Ph.D. student Taiting Liu from Penn State University gave a talk about his research projects on IoT devices. He introduced three projects: OmniRing, EARFace, and iBall.

The OminiRing project is designed for sensing human finger motions. It has wide applications, such as sign language recognition. The innovation part is that this ring has many sensors (MCU, IMU, etc) integrated into a very tiny ring-like device of a weight less than 3g. He showed a video demo where a hand with three rings can capture almost every tiny motions of each finger.

The EARFace project is to detect human facial expression based on a tiny device that can be placed in human's ears. The rationale behind is that facial expression usually triggers vibrations which can be detected in the ear canal. The EARFace includes an in-ear acoustic sensor which collects the vibration data. They feed the data into a machine learning model which extracts features of 8 different facial expressions. Results show that EARFace can recognize these 8 facial expressions accurately.

The last project is iBall, which is designed to track the cricket ball motions. The iBall is equipped with IMU sensors which helps track the 3D trajectory and speed of the ball.

He went pretty fast without jumping into detailed implementations. I asked one question concerning the OmniRing: what's the optimal number of rings should one wear in order to achieve good results in tracking finger motions? He responded that with just one ring it would work fairly well, but 3 rings are optimal in order to track subtle motions.


\newpage
\section{Talk on October 6, 2023}
\begin{tabularx} {\textwidth}{r X}
\textbf{Topic}: & Designing Quantum Programming Languages with Types \\
\textbf{Speaker:} & Frank (Peng) Fu\\
\end{tabularx}

Dr. Fu spent about 10 minutes introducing the motivations for designing quantum programming languages. He gave several examples of the application of quantum computing to justify the claim that it's the right time to think about quantum programming. For example, with quantum algorithm, factorizing big integers would be very feasible. 

He then pointed out his focus for the talk is about how to design a high-level programming language for quantum circuits. He also threw out the question why types are important in designing the programming language: to ensure type checking and maintaining invariants. He introduced basic typing rules which are generally true, and a type checker which evaluates  programs and does type checking. He also introduced some fancy types: linear type, dependent type, and type with modalities.

He went on to introduce types for quantum computing: Bit, Qubit, and Multi-qubit. He introduced the property that a Qubit is resource and can not be duplicated, Qubit should be initialized before use, and Qubit should be discarded explicitly at the end. He also introduced the unitary operation for Qubits.

He ran out of time and quickly wrapped up for the talk. But it's also interesting to learn the basic concepts of Qubits and operations.

When he was introducing the idealized typing rules, I asked one question: I believe these rules are generally true for any programing paradigm, not necessarily associated with the quantum programing context. He agreed. 


\newpage
\section{Talk on October 13, 2023}
\begin{tabularx} {\textwidth}{r X}
\textbf{Topic}: & Real-time Computing for Cyberphysical Systems \\
\textbf{Speaker:} & Dr. Yi Wang\\
\end{tabularx}

At the beginning of this talk, Dr. Wang directly introduced his research interests: Multiphysics Reduced Order Modeling, Data Analytics, and Real-time Computing for Cyber-physical Systems. He then introduced each area one by one.

The first field is Multiphysics Reduced Order Modeling. It's quite theoretical work. He explained the Equation-based Model Order Reduction in details. He introduced one example where there were over one million equations to solve, and with the reduction they can instead solve an equation system consisting of only 24 equations. He also introduced Data-driven and Surrogate Models.

He went on to introduce his research on Data Analytics, which involves massive computation. He gave an example of testing an aircraft flying in the space. When the aircraft is flying in the air, data from multi-source are collecting in real-time and be analyzing at real-time to send feedback to the pilot. I think the example give the audience an impression how data analytics is applied in real applications. He also introduced how data analytics and be employed for fault detection and diagnosis.

He finally introduced the third research field: Real-time Computing for Cyberphysical Systems. The application he introduced is autonomous car, where the focus is on real-time machine learning for resilient autonomous systems. He showed a demo of leader-follower control system, and went on to discuss the cyber-security issue with manufacturing systems. He also introduced a project which tries to decongestion traffic, and a project that detects abnormal behavior at real-time. He introduced several other projects including path planning for robotics, rail track inspection, etc. I believe the research challenge here is to collect data and process data in real-time, and make real-time decisions. 

Finally, he introduced a project about perception for unmanned maritime systems. He explained the challenge of identifying object on the sea: lighting condition, dynamic environment, etc. He showed a video about navigating his wireless-controlled boats on the lake. 

I asked one question: is it possible to detect object moving under the water? He responded that their purpose is to identify small static objects.

\newpage
\section{Talk on October 27, 2023}
\begin{tabularx} {\textwidth}{r X}
\textbf{Topic}: & Sensing the Future: Unveiling the Benefits and Risks of Sensing in Cyber-Physical Security \\
\textbf{Speaker:} & Dr. Jun Han\\
\end{tabularx}

At the beginning of this talk, Dr. Jun Han directly jumped into the security issues of cyber-physic systems. Have gave an example where two cars are in communication and raised the question of verifying one car's location. He then introduced the opportunities that exist in the signals emitted by the cars. He pointed out that some noises might also carry useful signals.

He then focused his talk on physics-guided attack detection. Specifically, he provided an example of counterfeit liquid food detection. The example is that it's difficult to verify whether a bottle contains true original olive oil, because it might be actually cheap liquid mixture of peanut oil and corn oil.  It is challenging to detect adulteration without open the bottles using only commodity devices. He then introduced his research work LiquidHash, which utilizes the bubble characteristics to infer the liquid authenticity. 

For LiquidHash to work,  challenges exist in multiple source of noise as well as minute difference of characteristics. Dr. Han's team designed the LiquidHash system, including pre-processing, feature extraction, and evaluation, to make it work effectively.

In the second half of talk, Dr. Han introduced his work in detecting hidden spy camera using laser time-of-flight (ToF) sensors on smartphones. He introduced the high level idea of the work and showed several screenshots of the app that they developed for detecting spy cameras. 

He continued to introduce the risks existing in the real world because of the signal-emitting in many real life scenarios. He introduced his research work of recovering a physical key's shape after capturing the sound when inserting the key into its lock. This research work appeared in news with title ``Picking locks with audio technology."

Finally, he introduced his research work on eavesdropping via lidar sensor. Specifically, a robot vacuum cleaner can be used as a spying device. I am also surprised that the lidar sensor can record conversations. This is actually quite related to one of my research projects.

I asked one question: for characterizing the liquid such as olive oil, is it possible for a person with a some training to characterize correctly by eye-looking? His response was that they did the investigation by inviting many people to do the experiment and the result was that human's eye-looking characterization usually got wrong.


\newpage
\section{Talk on November 3, 2023}
\begin{tabularx} {\textwidth}{r X}
\textbf{Topic}: &Data Annotation for Pervasive Sensing Systems: Challenges and Opportunities\\
\textbf{Speaker:} & Dr. Sandip Chakraborty\\
\end{tabularx}

Dr. Chakraborty started the talk by introducing multimodal sensing. He pointed out that in this research field people assume that the data they used in their research were annotated. He then jumped into the problem of annotating data efficiently. Have gave a concrete example that annotating 1000 image with two labels each might take 4 hours. He also pointed out that it's generally true that source data with similar annotations might be very different from different modalities.

He then posed the problem of how to automate annotating sensor streams. Traditionally people use video modality, which is computationally expensive and privacy invasive. He proposed to build acoustic signature for a sense, based on the fact that it's quite practical to detect activities with certain granularity.  The proposed method is to detect changes in IMU due to physical activity and check corresponding acoustic signatures and detect activities from the acoustic signatures. The proposed method is practical because that IMU can help detect precise activities, and there is a rich resource to collect acoustic signatures from youtube.

The proposed method is called LASO, which achieved great accuracy in two environments: kitchen environment and workshop environment. 

He then introduced his work to solve another challenge: user to activity association. In a scenario where some people are doing cooking and chopping, how to identify that who is cooking?

The proposed framework is called Acconotate, which exploits the acoustic gap in the audio. Detect the change in IMU data, detect acoustic gaps, and then search for user who entered acoustic gap and use the audio-based activity recognition module to perform activity association. He also discussed how to annotate the unlabeled IMU data. 
Acconate achieved accuracy of 82\% for a workshop environment and 98\% for a kitchen environment. 

He then continued to introducing Human-in-the-loop Annotation. He threw out the problem that the annotation might not exactly reflect the activities in the scene. He also spent some time introducing explainability of annotations. 

I asked one question: what are the reasons that in evaluation two environment (kitchen, workshop) are chosen? What are the challenges if the proposed solutions are applied to environments other than kitchen or workshop? He's response was that these two environments (kitchen and workshop) generates rich sound that would help identify activities, and they are looking for other environments that would be suitable for the evaluation.



\end{document} 
